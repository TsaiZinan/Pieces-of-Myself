# GPT实践

## 2023 06 14

今天想把之前所有的微博保存成Markdown文件，借助GPT的帮助短短一天内完成，放之前至少要四五天，感叹GPT确实是一个极大提升生产力的工具。

首先用了[weibo-crawler](https://github.com/dataabc/weibo-crawler)这一个工具把微博所有的内容下载下来。下下来是一个JSON文件，里面充满了各种数据，于是求助GPT写了一个Python程序，把每条微博的时间，文字和图片提取出来，按特定的格式写入一个Markdown文件。

```{python}
import json

def json_to_md(json_file, md_file):
    with open(json_file, 'r', encoding='utf-8') as jf:
        data = json.load(jf)

    with open(md_file, 'w', encoding='utf-8') as mf:
        for item in data:
            date = item['full_created_at'].split(' ')[0].replace('-', ' ')
            text = item['text']
            pics = item['pics'].replace('/large', '/mw2000').split(',') if item['pics'] else []

            mf.write(f'{date}\n')
            mf.write(f'#### {text}\n')
            for pic in pics:
                mf.write(f'![]({pic})\n')
            mf.write('\n')


json_to_md('input.json', 'output.md')
```

这个程序挺简单。本来以为任务完成，但当把整个md文件上传到网站上后，发现所有的图片都无法加载出来。进入控制台发现因为整个文件有500多张图片，存在微博的图床上。因为同时发出太多请求，所以被拒绝访问代码403。

于是现在只能把文件里所有图片链接换到imgur里，步骤大概就是：

1. 读取图片链接
2. 下载图片
3. 上传到imgur
4. 拿到imgur的图片链接
5. 替换原来的图片链接

在跟GPT来来回回几个回合之后，写出了第一版程序。结果发现imgur有上传单位时间内的数量限制，于是加上了时间间隔。

重新运行，还是遇到同样的问题，发现是因为imgur同一个IP一小时只能50个上传请求。于是只能加上进度保存断点续传功能，再配合上切VPN，最终终于把500多张图片替换完毕。

在写这个程序就比较波折了，就算有GPT的帮助，也要**17轮对话**逐步DEBUG才得到最终的程序。里面的一个BUG还是自己发现之后告知GPT，它才改过来。

## 总结

总的来说，对GPT来说，简单的程序一般不难写出来。稍微复杂点的，最好自己要知道如何分解程序的实现步骤，引导GPT逐步去写。同时自己如果能大概定位BUG的位置，花费的时间会大大减少。

下面是整个程序：

```{python}
import re
import time
from imgurpython import ImgurClient
from imgurpython.helpers.error import ImgurClientRateLimitError

# Get the API keys from Imgur
client_id = 'Your Client ID'
client_secret = 'Your Client Secret'

# Create an Imgur client
client = ImgurClient(client_id, client_secret)

# Read the Markdown file
with open('new_markdown_file.md', 'r', encoding='utf-8') as file:
    lines = file.readlines()

# First, calculate the total number of images
total_images = sum('![' in line for line in lines)

# Read the last progress
try:
    with open('progress.txt', 'r') as file:
        last_progress = int(file.readline())
except (FileNotFoundError, ValueError):
    last_progress = 0

# Number of images already processed, starting from the last progress
processed_images = sum('![' in line for line in lines[:last_progress])

# Traverse each line, starting from the last progress
for i, line in enumerate(lines, start=1):
    # If the current line number is less than or equal to the last progress, skip it
    if i <= last_progress:
        continue

    # Check if this line contains an image link
    match = re.search(r'!\[\]\((.*)\)', line)
    if match:
        # Extract the image link
        url = match.group(1)

        try:
            # Upload the image from the URL to Imgur
            response = client.upload_from_url(url, config=None, anon=True)

            # Get the new Imgur link
            new_url = response['link']

            # Replace the original link
            new_line = line.replace(url, new_url)

            # Update this line
            lines[i - 1] = new_line

            # The image conversion is complete, increase the number of processed images
            processed_images += 1

            # Print the completed information and progress
            print(f"Image {processed_images}/{total_images} converted, progress: {processed_images / total_images * 100:.2f}%")

            # Save after each image is processed
            with open('new_markdown_file.md', 'w', encoding='utf-8') as file:
                file.writelines(lines)
            with open('progress.txt', 'w') as file:
                file.write(str(i))

            # Wait for 8 seconds to avoid triggering Imgur's rate limit
            time.sleep(8)
        except ImgurClientRateLimitError:
            print("Rate-limit exceeded! Please change your IP to avoid the upload limit.")
            break
    # Print the number of processed lines
    print(f"Processed lines: {i}")

# Save the new Markdown file
with open('new_markdown_file.md', 'w', encoding='utf-8') as file:
    file.writelines(lines)

```